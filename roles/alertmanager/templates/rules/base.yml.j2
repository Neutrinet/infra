#jinja2: variable_start_string:'{{{', variable_end_string: '}}}'
{{{ ansible_managed | comment }}}

groups:
  - name: Health
    rules:
      - alert: Instance Down
        for: 5m
        expr: |-
{% for host in groups.all %}
          absent(static_heartbeat{instance="{{{ host }}}"}) == 1{% if not loop.last %} OR {% endif %}

{% endfor %}
        labels:
          severity: critical
        annotations:
          summary: 'Instance is down'
          description: '[{{ $labels.instance }}] Instance is down'

  - name: Disk Usage
    rules:
      - alert: Low Disk Space
        for: 1h
        expr: (1 - node_filesystem_avail_bytes{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"} / node_filesystem_size_bytes) > 0.9 and node_filesystem_readonly == 0
        labels:
          severity: critical
        annotations:
          summary: 'Disk usage is at {{ humanizePercentage $value }}%'
          description: '[{{ $labels.instance }}] Device {{ $labels.device }} mounted on "{{ $labels.mountpoint }}" has disk usage > 90% (current = {{ humanizePercentage $value }}%)'

      - alert: Disk Full Next Week
        for: 1h
        expr: (1 - node_filesystem_avail_bytes{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"} / node_filesystem_size_bytes) > 0.8 and predict_linear(node_filesystem_avail_bytes{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"}[2d], 7 * 24 * 3600) <= 0 and node_filesystem_avail_bytes > 0
        labels:
          severity: warning
          frequency: daily
        annotations:
          summary: 'Disk full within the week (current = {{ humanizePercentage $value }}%)'
          description: '[{{ $labels.instance }}] Based on disk usage from last 2 days, device {{ $labels.device }} mounted on "{{ $labels.mountpoint }}" will be full within the week (current = {{ humanizePercentage $value }}%)'

      - alert: Low Inodes
        for: 1h
        expr: (1 - node_filesystem_files_free / node_filesystem_files) > 0.9 and node_filesystem_readonly == 0
        labels:
          severity: critical
        annotations:
          summary: 'Inodes usage is at {{ humanizePercentage $value }}%'
          description: '[{{ $labels.instance }}] Device {{ $labels.device }} mounted on "{{ $labels.mountpoint }}" has inodes usage > 90% (current = {{ humanizePercentage $value }}%)'

      - alert: Inodes Full Next Week
        for: 1h
        expr: (1 - node_filesystem_files_free / node_filesystem_files) > 0.9 and predict_linear(node_filesystem_files_free{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"}[2d], 7 * 24 * 3600) <= 0 and node_filesystem_files_free > 0
        labels:
          severity: warning
          frequency: daily
        annotations:
          summary: 'Inodes full within the week (current = {{ humanizePercentage $value }}%)'
          description: '[{{ $labels.instance }}] Based on inodes usage from last 2 days, device {{ $labels.device }} mounted on "{{ $labels.mountpoint }}" will be full within the week (current = {{ humanizePercentage $value }}%)'

      - alert: Filesystem Device Error
        expr: node_filesystem_device_error{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"} == 1
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: Filesystem device error on device {{ $labels.device }} ({{ $labels.device_error }})
          description: "[{{ $labels.instance }}] Filesystem error found on device {{ $labels.device }} mounted on {{ $labels.mountpoint }} ({{ $labels.device_error }})"

  - name: Memory Usage
    rules:
      - alert: High Memory Usage
        for: 15m
        expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) > 0.9
        labels:
          severity: critical
        annotations:
          summary: 'Memory usage is at {{ humanizePercentage $value }}%'
          description: '[{{ $labels.instance }}] Memory usage is > 90% (current = {{ humanizePercentage $value }}%)'

      - alert: High Memory Major Page Faults
        expr: rate(node_vmstat_pgmajfault[5m]) > 500
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: 'Memory page faults occurring at high rate (current = {{ humanize $value }})'
          description: "[{{ $labels.instance }}] Instance is under heavy memory pressure. High rate of loading memory pages from disk (current = {{ humanize $value }})"

      - alert: Memory Under Utilized
          expr: (1 - avg_over_time(node_memory_MemFree_bytes[1d]) / node_memory_MemTotal_bytes) < 0.4
          for: 1h
          labels:
            severity: info
            frequency: weekly
          annotations:
            summary: 'Memory is underutilized (current = {{ humanizePercentage $value }}%)'
            description: "[{{ $labels.instance }}] Memory usage is < 40% for 1 week (current = {{ humanizePercentage $value }}%). Consider reducing available memory space"

      - alert: High Swap Usage
        for: 15m
        expr: (1 - node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes) > 0.8
        labels:
          severity: warning
        annotations:
          summary: Swap usage is at {{ humanizePercentage $value }}%
          description: "[{{ $labels.instance }}] Swap usage is > 80% (current = {{ humanizePercentage $value }}"

      - alert: OOM Kill Detected
        expr: increase(node_vmstat_oom_kill) > 0
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: OOM kill detected
          description: "[{{ $labels.instance }}] OOM kill detected."

  - name: CPU Usage
    rules:
      - alert: High CPU Usage
        for: 15m
        expr: node_load5 / count without (cpu, mode) (node_cpu_seconds_total{mode="idle"}) > 2
        labels:
          severity: warning
        annotations:
          summary: CPU usage is at {{ humanizePercentage $value }}%
          description: '[{{ $labels.instance }}] CPU usage is > 200% (value = {{ humanizePercentage $value }}%)'

      - alert: CPU Steal Noisy Neighbor
        expr: avg without (cpu, mode) (rate(node_cpu_seconds_total{mode="steal"}[5m])) > 0.1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: CPU steal increased by {{ humanizePercentage $value }} in the last 5 minutes.
          description: "[{{ $labels.instance }}] CPU steal increased by > 10% in the last 5 minutes (current = {{ humanizePercentage $value }}%). A noisy neighbor is killing VM performances or a spot instance may be out of credit."

  - name: Network
    rules:
      - alert: Unusual Network Receive Bandwidth
        expr: rate(node_network_receive_bytes_total[5m]) / node_network_speed_bytes > 0.8
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Network receive bandwidth is at {{ humanizePercentage $value }}%
          description: "[{{ $labels.instance }}] Receive bandwidth is > 80% (current = {{ humanizePercentage $value }}%)"

      - alert: Unusual Network Transmit Bandwidth
        expr: rate(node_network_transmit_bytes_total[5m]) / node_network_speed_bytes > 0.8
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Network transmit bandwidth is at {{ humanizePercentage $value }}%
          description: "[{{ $labels.instance }}] Transmit bandwidth is > 80% (current = {{ humanizePercentage $value }}%)"

      - alert: Network Receive Errors
        expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Network receive errors is at {{ humanizePercentage $value }}%
          description: "[{{ $labels.instance }}] Interface {{ $labels.device }} has encountered {{ humanizePercentage $value }}% receive errors in the last 2 minutes."

      - alert: Network Transmit Errors
        expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Network transmit errors is at {{ humanizePercentage $value }}%
          description: "[{{ $labels.instance }}] Interface {{ $labels.device }} has encountered {{ humanizePercentage $value }}% transmit errors in the last 2 minutes."

      - alert: Network Bond Degraded
        expr: (node_bonding_active - node_bonding_slaves) != 0
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Network bond is degraded
          description: "[{{ $labels.instance }}] Bond \"{{ $labels.master }}\" is degraded (current = {{ $value }})"

      - alert: Conntrack Limit
        expr: node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Number of conntrack is at {{ humanizePercentage $value }}%
          description: "[{{ $labels.instance }}] Number of conntrack is > {{ humanizePercentage $value }}% than the limit."

  - name: System
    rules:
      - alert: Systemd Service Failed
        expr: node_systemd_unit_state{state="failed", name!~"borgmatic.*\.service"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Systemd service {{ $labels.name }} failed
          description: "[{{ $labels.instance }}] Systemd service {{ $labels.name }} failed."

      - alert: Kernel Version Changed
        expr: changes(node_uname_info[1h]) > 0
        for: 1m
        labels:
          severity: info
        annotations:
          summary: Kernel version changed
          description: "[{{ $labels.instance }}] Kernel version has changed (current = {{ $labels.release }})"

  - name: Time
    rules:
      - alert: Clock Skew
        expr: abs(node_timex_offset_seconds) > 0.05 and abs(deriv(node_timex_offset_seconds[5m])) >= 0
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Clock skew detected
          description: "[{{ $labels.instance }}] Clock skew detected. Clock is out of sync. Ensure NTP is configured correctly on this host (current = {{ humanize $value }})"

      - alert: Clock Not Synchronising
        expr: min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16'
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Clock is not synchronising
          description: "[{{ $labels.instance }}] Clock is not synchronising. Ensure NTP is configured on this host."

  - name: Hardware
    rules:
      - alert: Physical Component Too Hot
        for: 5m
        expr: node_hwmon_temp_celsius / node_hwmon_temp_max_celsius > 0.9
        labels:
          severity: warning
        annotations:
          summary: Physical component {{ $labels.chip }} is too hot (current = {{ humanizePercentage $value }}%)
          description: "[{{ $labels.instance }}] Physical hardware component {{ $labels.chip }} is > 90% than max temperature (current = {{ humanizePercentage $value }}%, sensor = {{ $labels.sensor }})"

      - alert: Temperature Alarm
        expr: (node_hwmon_temp_celsius / node_hwmon_temp_critical_celsius > 0.9) or node_hwmon_temp_alarm == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Physical component {{ $labels.chip }} temperature is critical (current = {{ humanizePercentage $value }}%)
          description: "[{{ $labels.instance }}] Physical hardware component {{ $labels.chip }} is > 90% than critical temperature or the critical alerm triggered (current = {{ humanizePercentage $value }}%, sensor = {{ $labels.sensor }})"

      - alert: Software Raid Insufficient Drives
        for: 5m
        expr: (node_md_disks_required - ignoring (state) node_md_disks{state="active"}) > 0
        labels:
          severity: critical
          frequency: weekly
        annotations:
          summary: Software RAID has insufficient drives (missing = {{ $value }})
          description: "[{{ $labels.instance }}] Software RAID array {{ $labels.device }} has insufficient drives remaining (missing = {{ $value }})"

      - alert: Software Raid Disk Failure
        for: 5m
        expr: node_md_disks{state="failed"} > 0
        labels:
          severity: warning
          frequency: weekly
        annotations:
          summary: Software RAID has {{ $value }} failed disks.
          description: "[{{ $labels.instance }}] MD RAID array {{ $labels.device }} has {{ $value }} failed disks."

      - alert: Disk failure
        for: 5m
        expr: smartctl_device_smart_status != 1
        labels:
          severity: critical
          frequency: weekly
        annotations:
          summary: 'Disk /dev{{ $labels.device }} is failing'
          description: '[{{ $labels.host }}] Disk /dev/{{ $labels.device }} is failing'

      - alert: Disk Low Wear Level
        for: 5m
        expr: smartctl_device_attribute{attribute_name="Wear_Leveling_Count", attribute_value_type="value"} < 10
        labels:
          severity: critical
          frequency: daily
        annotations:
          summary: 'Disk /dev{{ $labels.device }} wear level is at {{ $value }}%'
          description: '[{{ $labels.instance }}] Disk /dev/{{ $labels.device }} wear level is < 10% (current = {{ $value }})'

      - alert: Disk Failure Next Month
        for: 5m
        expr: 0 < smartctl_device_attribute{attribute_name="Wear_Leveling_Count", attribute_value_type="value"} <= 20 and predict_linear(smartctl_device_attribute{attribute_name="Wear_Leveling_Count", attribute_value_type="value"}[30d], 30 * 24 * 3600) <= 0
        labels:
          severity: warning
          frequency: daily
        annotations:
          summary: 'Disk /dev{{ $labels.device }} will fail in 30 days (current wear level = {{ $value }}%)'
          description: '[{{ $labels.instance }}] Disk /dev/{{ $labels.device }} will fail in 30 days (current wear level = {{ $value }}%)'

      - alert: EDAC Correctable Errors Detected
        expr: increase(node_edac_correctable_errors_total[5m]) > 0
        for: 0m
        labels:
          severity: info
        annotations:
          summary: EDAC correctable memory errors detected (current = {{ humanize $value }})
          description: "[{{ $labels.instance }}] Detected {{ humanize $value }} correctable memory errors reported by EDAC in the last 5 minutes."

      - alert: EDAC Uncorrectable Errors Detected
        expr: increase(node_edac_uncorrectable_errors_total[5m]) > 0
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: EDAC uncorrectable memory errors detected (current = {{ humanize $value }})
          description: "[{{ $labels.instance }}] Detected {{ humanize $value }} uncorrectable memory errors reported by EDAC in the last 5 minutes."

  - name: Disk IO
    rules:
      - alert: Unusual Disk IO
        expr: rate(node_disk_io_time_seconds_total[5m]) > 1
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Unusual disk IO rate (current = {{ humanize $value }})
          description: "[{{ $labels.instance }}] Disk {{ $labels.device }} is too busy: IO wait increased by > 100% in the last 5 minutes (current = {{ humanize $value }})."

      - alert: Unusual Disk Read Latency
        expr: rate(node_disk_read_time_seconds_total[5m]) / rate(node_disk_reads_completed_total[5m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Unusual disk read latency (current {{ humanize $value }})
          description: "[{{ $labels.instance }}] Disk {{ $labels.device }} read latency is growing (read operations > 100ms)"

      - alert: Unusual Disk Write Latency
        expr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0)'
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Unusual disk write latency (current {{ humanize $value }})
          description: "[{{ $labels.instance }}] Disk {{ $labels.device }} write latency is growing (write operations > 100ms)"

  - name: Probe
    rules:
      - alert: Certificate Expires Next Month
        for: 5m
        expr: round((probe_ssl_earliest_cert_expiry{check_cert_expiry="1"} - time()) / 86400) < 28
        labels:
          severity: critical
          frequency: daily
          category: probe.cert
        annotations:
          summary: 'Certificate for url {{ $labels.module }} will expire within the month'
          description: '[{{ $labels.module }}] Certificate will expire in {{ $value }} days'

      - alert: Website Down
        for: 5m
        expr: probe_success{type="http"} == 0
        labels:
          severity: critical
          category: probe.http
        annotations:
          summary: 'Website {{ $labels.module }} is down'
          description: '[{{ $labels.module }}] Website is down'

      - alert: Website HTTP Failure
        expr: 0 < probe_http_status_code <= 199 OR probe_http_status_code >= 400
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Website {{ $labels.module }} HTTP status code is {{ $value }}
          description: "[{{ $labels.module }}] HTTP status code is {{ $value }}"

      - alert: Website Slow
        expr: avg_over_time(probe_http_duration_seconds[10m]) > 5
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Website {{ $labels.module }} is slow to respond
          description: "[{{ $labels.module }}] Host took > 5s to respond in the last 10min (current = {{ humanize $value }}s)"
      avg_over_time(probe_duration_seconds[5m]) > 5

      - alert: Ping Down
        for: 5m
        expr: probe_success{module="icmp"} == 0
        labels:
          severity: critical
          category: probe.icmp
        annotations:
          summary: "Host {{ $labels.url }} doesn't respond to ping"
          description: "[{{ $labels.url }}] Host doesn't respond to ping"

      - alert: Ping Slow
        expr: avg_over_time(probe_icmp_duration_seconds[1m]) > 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host {{ $labels.url }} is slow to respond to ping
          description: "[{{ $labels.url }}] Host took > 1s to respond to ping in the last minute (current = {{ humanize $value }}s)"
      avg_over_time(probe_duration_seconds[5m]) > 5

  - name: Backup
    rules:
      - alert: Backup Borgmatic Failed
        for: 5m
        # We trigger an alert only if several backups failed within 24h.
        # It's not a problem if we miss one or two backups during the day, since we create a backup every 3h.
        # In short, we trigger an alert if more than 4 backups failed (24h / 3h = 8 backups per day)
        expr: sum_over_time(node_systemd_unit_state{name="borgmatic.service", state="failed"}[1d]) / ignoring (state) sum without (state) (sum_over_time(node_systemd_unit_state{name="borgmatic.service"}[1d])) > 0.5
        labels:
          severity: warning
          frequency: daily
        annotations:
          summary: 'Borgmatic backups failed within 24h'
          description: '[{{ $labels.instance }}] Borgmatic backups failed several times within 24h (failure ratio = {{ humanizePercentage $value }}%)'
